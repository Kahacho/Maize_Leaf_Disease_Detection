{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-01T17:21:37.608808800Z",
     "start_time": "2023-10-01T17:21:33.887078600Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import ImageFile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# To avoid PIL throwing the error \"OSError: image file is truncated\". \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Define the data path\n",
    "data_path = Path(\"__file__\").cwd() / \"maize_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaders ready!\n"
     ]
    }
   ],
   "source": [
    "class LoadDataset:\n",
    "    \"\"\"\n",
    "    Class to ingest data using training and test loaders. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        # Define Data Transformation\n",
    "        self.transformation = transforms.Compose([\n",
    "            # Resize to a common 128 * 128 image size\n",
    "            transforms.Resize(size=[128, 128]),\n",
    "            # Transform to tensors\n",
    "            transforms.ToTensor(),\n",
    "            # Normalize the pixel values (in R, G, and B channels)\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "        # Load all the images, transforming them\n",
    "        self.full_dataset = ImageFolder(root=data_path, transform=self.transformation)\n",
    "\n",
    "    def data_classes(self):\n",
    "        \"\"\"Get the class and class index\"\"\"\n",
    "        classes = self.full_dataset.classes\n",
    "        classes_indx = self.full_dataset.class_to_idx\n",
    "\n",
    "        return classes, classes_indx\n",
    "\n",
    "    def data_loaders(self, train_size=0.7):\n",
    "        \"\"\" Get the iterative data loaders for test and training data \"\"\"\n",
    "\n",
    "        # Split into training (70%) and testing (30%) datasets\n",
    "        train_size = int(train_size * len(self.full_dataset))\n",
    "        test_size = len(self.full_dataset) - train_size\n",
    "        # generator = torch.manual_seed(42)  # Set the seed to ensure consistent results\n",
    "        train_dataset, test_dataset = random_split(dataset=self.full_dataset, \n",
    "                                                   lengths=[train_size, test_size],\n",
    "                                                   # generator=generator\n",
    "                                                   )\n",
    "\n",
    "        # Define a loader for the training data we can iterate through in 50-image batches\n",
    "        train_loader = DataLoader(dataset=train_dataset,\n",
    "                                  batch_size=50,\n",
    "                                  num_workers=0,\n",
    "                                  shuffle=False)\n",
    "\n",
    "        # Define a loader for the testing data we can iterate through in 50-image batches\n",
    "        test_loader = DataLoader(dataset=test_dataset,\n",
    "                                 batch_size=50,\n",
    "                                 num_workers=0,\n",
    "                                 shuffle=False)\n",
    "\n",
    "        return train_loader, test_loader \n",
    "    \n",
    "# Get the iterative data loaders for test and training data\n",
    "load_dataset = LoadDataset(data_path=data_path)\n",
    "train_loader, test_loader = load_dataset.data_loaders()\n",
    "classes, class_indexes = load_dataset.data_classes()\n",
    "print('Data loaders ready!')   "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-01T17:21:37.885610800Z",
     "start_time": "2023-10-01T17:21:37.611827200Z"
    }
   },
   "id": "f89612619fe563d1"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN model class defined!\n"
     ]
    }
   ],
   "source": [
    "# Define a CNN Model\n",
    "class Net(nn.Module):\n",
    "    \"\"\" Create a neural net class \"\"\"\n",
    "    def __init__(self, num_classes=3):\n",
    "        # num_classes is 3 because we have 3 classes: Health, MLN, and MSV\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Our images are RGB, so input_channels=3. \n",
    "        # We'll apply 12 filters in the first convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=3,\n",
    "                               out_channels=12,\n",
    "                               kernel_size=3,\n",
    "                               stride=1,\n",
    "                               padding=1)\n",
    "        \n",
    "        # We'll apply max pooling with a kernel size of 2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # A second convolution layer takes 12 input channels, and generates 12 outputs\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, \n",
    "                               out_channels=12,\n",
    "                               kernel_size=3,\n",
    "                               stride=1, \n",
    "                               padding=1)\n",
    "        \n",
    "        # A third convolutional layers takes 12 inputs and generates 24 outputs\n",
    "        self.conv3 = nn.Conv2d(in_channels=12,\n",
    "                               out_channels=24,\n",
    "                               kernel_size=3,\n",
    "                               stride=1,\n",
    "                               padding=1)\n",
    "        \n",
    "        # A drop layer deletes 20% of the features to help prevent overfitting\n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "        \n",
    "        # Our 128*128 image tensors will be pooled twice with a kernel size 0f 2. \n",
    "        # 128/2/2 is 32. So our feature tensors are now 32 * 32, and we have generated 24 of them. \n",
    "        # We need to flatten these and feed them to a fully-connected layer to map them to the probability of each class\n",
    "        self.fc = nn.Linear(in_features=32 * 32 * 24, out_features=num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Use a ReLu activation function after layer 1 (convolution 1 and pool)\n",
    "        x = F.relu(self.pool(self.conv1(x)))\n",
    "        \n",
    "        # Use a ReLu activation function after layer 2 (convolution 2 and pool)\n",
    "        x = F.relu(self.pool(self.conv2(x)))\n",
    "        \n",
    "        # Select some features to drop after the 3rd convolution to prevent overfitting\n",
    "        x = F.relu(self.drop(self.conv3(x)))\n",
    "        \n",
    "        # Only drop the features if this is a training pass. \n",
    "        # By default, self.training flag is set to True, i.e. modules are in train mode by default\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(-1, 32 * 32 * 24)\n",
    "        \n",
    "        # Feed to fully-connected layer to predict class\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # Return log_softmax tensor\n",
    "        return F.log_softmax(x, dim=1)\n",
    "        \n",
    "print(\"CNN model class defined!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-01T17:21:37.896980900Z",
     "start_time": "2023-10-01T17:21:37.885610800Z"
    }
   },
   "id": "6b565be4330b60e1"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    # Process the images in batches\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Use the CPU or GPU as appropriate\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Reset the optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Push the data forward through the model layers\n",
    "        output = model(data)\n",
    "\n",
    "        # Get the loss\n",
    "        loss = loss_criteria(output, target)\n",
    "\n",
    "        # Keep a running total\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print metrics for every 10 batches, so we see some progress\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(\n",
    "                f\"Training set [{batch_idx * len(data)}/{len(train_loader.dataset)} \"\n",
    "                f\"({100. * batch_idx / len(train_loader):.0f}%)] Loss: {loss.item():.6F}\")\n",
    "\n",
    "    # Return average loss for the epoch\n",
    "    avg_loss = train_loss / (batch_idx + 1)\n",
    "    print(f\"Training set: Average loss: {avg_loss:.6f}\")\n",
    "\n",
    "    return avg_loss "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-01T17:21:37.947240800Z",
     "start_time": "2023-10-01T17:21:37.897763100Z"
    }
   },
   "id": "80d5a51494794f88"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    # Switch the model to evaluation mode (so we don't propagate or drop)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    # Context manager to disable gradient calculation\n",
    "    with torch.no_grad():\n",
    "        batch_count = 0\n",
    "        for data, target in test_loader:\n",
    "            batch_count += 1\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Get the predicted classes for this batch\n",
    "            output = model(data)\n",
    "            \n",
    "            # Calculate the loss for this batch\n",
    "            test_loss += loss_criteria(output, target).item()\n",
    "            \n",
    "            # Calculate the accuracy for this batch\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            correct += torch.sum(target==predicted).item()\n",
    "    \n",
    "    # Calculate the average loss and total accuracy for this epoch\n",
    "    avg_loss = test_loss/batch_count\n",
    "    print(f\"Validation set: Average loss: {avg_loss:.6f}, \"\n",
    "          f\"Accuracy: {correct}/{len(test_loader.dataset)} \"\n",
    "          f\"({100. * correct / len(test_loader.dataset):.0f}%\\n)\")\n",
    "    \n",
    "    # Return average loss for the epoch\n",
    "    return  avg_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-01T17:21:37.951756100Z",
     "start_time": "2023-10-01T17:21:37.913455200Z"
    }
   },
   "id": "9d1c221605f65cc6"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device!\n",
      "Epoch: 1\n",
      "Training set [0/10745 (0%)] Loss: 1.094694\n",
      "Training set [500/10745 (5%)] Loss: 1.090229\n",
      "Training set [1000/10745 (9%)] Loss: 1.095080\n",
      "Training set [1500/10745 (14%)] Loss: 1.092264\n",
      "Training set [2000/10745 (19%)] Loss: 1.073338\n",
      "Training set [2500/10745 (23%)] Loss: 1.033788\n",
      "Training set [3000/10745 (28%)] Loss: 1.030810\n",
      "Training set [3500/10745 (33%)] Loss: 1.026028\n",
      "Training set [4000/10745 (37%)] Loss: 1.044840\n",
      "Training set [4500/10745 (42%)] Loss: 1.019955\n",
      "Training set [5000/10745 (47%)] Loss: 1.059219\n",
      "Training set [5500/10745 (51%)] Loss: 1.016836\n",
      "Training set [6000/10745 (56%)] Loss: 1.025625\n",
      "Training set [6500/10745 (60%)] Loss: 1.067237\n",
      "Training set [7000/10745 (65%)] Loss: 0.975223\n",
      "Training set [7500/10745 (70%)] Loss: 1.019793\n",
      "Training set [8000/10745 (74%)] Loss: 1.033744\n",
      "Training set [8500/10745 (79%)] Loss: 1.020544\n",
      "Training set [9000/10745 (84%)] Loss: 1.002825\n",
      "Training set [9500/10745 (88%)] Loss: 0.987522\n",
      "Training set [10000/10745 (93%)] Loss: 0.982856\n",
      "Training set [10500/10745 (98%)] Loss: 1.032583\n",
      "Training set: Average loss: 1.039158\n",
      "Validation set: Average loss: 0.981885, Accuracy: 2557/4605 (56%\n",
      ")\n",
      "Epoch: 2\n",
      "Training set [0/10745 (0%)] Loss: 1.013794\n",
      "Training set [500/10745 (5%)] Loss: 0.947435\n",
      "Training set [1000/10745 (9%)] Loss: 0.964832\n",
      "Training set [1500/10745 (14%)] Loss: 1.013689\n",
      "Training set [2000/10745 (19%)] Loss: 1.000557\n",
      "Training set [2500/10745 (23%)] Loss: 0.881551\n",
      "Training set [3000/10745 (28%)] Loss: 0.937374\n",
      "Training set [3500/10745 (33%)] Loss: 0.906265\n",
      "Training set [4000/10745 (37%)] Loss: 0.972124\n",
      "Training set [4500/10745 (42%)] Loss: 0.927641\n",
      "Training set [5000/10745 (47%)] Loss: 0.994054\n",
      "Training set [5500/10745 (51%)] Loss: 0.914805\n",
      "Training set [6000/10745 (56%)] Loss: 0.924665\n",
      "Training set [6500/10745 (60%)] Loss: 0.989898\n",
      "Training set [7000/10745 (65%)] Loss: 0.847621\n",
      "Training set [7500/10745 (70%)] Loss: 0.894028\n",
      "Training set [8000/10745 (74%)] Loss: 0.936278\n",
      "Training set [8500/10745 (79%)] Loss: 0.890855\n",
      "Training set [9000/10745 (84%)] Loss: 0.855194\n",
      "Training set [9500/10745 (88%)] Loss: 0.885877\n",
      "Training set [10000/10745 (93%)] Loss: 0.878665\n",
      "Training set [10500/10745 (98%)] Loss: 0.974257\n",
      "Training set: Average loss: 0.932722\n",
      "Validation set: Average loss: 0.865935, Accuracy: 2836/4605 (62%\n",
      ")\n",
      "Epoch: 3\n",
      "Training set [0/10745 (0%)] Loss: 0.881700\n",
      "Training set [500/10745 (5%)] Loss: 0.767247\n",
      "Training set [1000/10745 (9%)] Loss: 0.842400\n",
      "Training set [1500/10745 (14%)] Loss: 0.888400\n",
      "Training set [2000/10745 (19%)] Loss: 0.901132\n",
      "Training set [2500/10745 (23%)] Loss: 0.761155\n",
      "Training set [3000/10745 (28%)] Loss: 0.816931\n",
      "Training set [3500/10745 (33%)] Loss: 0.782234\n",
      "Training set [4000/10745 (37%)] Loss: 0.841674\n",
      "Training set [4500/10745 (42%)] Loss: 0.822423\n",
      "Training set [5000/10745 (47%)] Loss: 0.878420\n",
      "Training set [5500/10745 (51%)] Loss: 0.805115\n",
      "Training set [6000/10745 (56%)] Loss: 0.809318\n",
      "Training set [6500/10745 (60%)] Loss: 0.908532\n",
      "Training set [7000/10745 (65%)] Loss: 0.701676\n",
      "Training set [7500/10745 (70%)] Loss: 0.774337\n",
      "Training set [8000/10745 (74%)] Loss: 0.822045\n",
      "Training set [8500/10745 (79%)] Loss: 0.780710\n",
      "Training set [9000/10745 (84%)] Loss: 0.702284\n",
      "Training set [9500/10745 (88%)] Loss: 0.745495\n",
      "Training set [10000/10745 (93%)] Loss: 0.740999\n",
      "Training set [10500/10745 (98%)] Loss: 0.829230\n",
      "Training set: Average loss: 0.809067\n",
      "Validation set: Average loss: 0.719756, Accuracy: 3319/4605 (72%\n",
      ")\n",
      "Epoch: 4\n",
      "Training set [0/10745 (0%)] Loss: 0.728222\n",
      "Training set [500/10745 (5%)] Loss: 0.595601\n",
      "Training set [1000/10745 (9%)] Loss: 0.681690\n",
      "Training set [1500/10745 (14%)] Loss: 0.717159\n",
      "Training set [2000/10745 (19%)] Loss: 0.748832\n",
      "Training set [2500/10745 (23%)] Loss: 0.604536\n",
      "Training set [3000/10745 (28%)] Loss: 0.672458\n",
      "Training set [3500/10745 (33%)] Loss: 0.636752\n",
      "Training set [4000/10745 (37%)] Loss: 0.619979\n",
      "Training set [4500/10745 (42%)] Loss: 0.639166\n",
      "Training set [5000/10745 (47%)] Loss: 0.697801\n",
      "Training set [5500/10745 (51%)] Loss: 0.658997\n",
      "Training set [6000/10745 (56%)] Loss: 0.671517\n",
      "Training set [6500/10745 (60%)] Loss: 0.718769\n",
      "Training set [7000/10745 (65%)] Loss: 0.523807\n",
      "Training set [7500/10745 (70%)] Loss: 0.665911\n",
      "Training set [8000/10745 (74%)] Loss: 0.630453\n",
      "Training set [8500/10745 (79%)] Loss: 0.648222\n",
      "Training set [9000/10745 (84%)] Loss: 0.518479\n",
      "Training set [9500/10745 (88%)] Loss: 0.601397\n",
      "Training set [10000/10745 (93%)] Loss: 0.622057\n",
      "Training set [10500/10745 (98%)] Loss: 0.640976\n",
      "Training set: Average loss: 0.651181\n",
      "Validation set: Average loss: 0.575213, Accuracy: 3677/4605 (80%\n",
      ")\n",
      "Epoch: 5\n",
      "Training set [0/10745 (0%)] Loss: 0.545667\n",
      "Training set [500/10745 (5%)] Loss: 0.446660\n",
      "Training set [1000/10745 (9%)] Loss: 0.536361\n",
      "Training set [1500/10745 (14%)] Loss: 0.564965\n",
      "Training set [2000/10745 (19%)] Loss: 0.590480\n",
      "Training set [2500/10745 (23%)] Loss: 0.453839\n",
      "Training set [3000/10745 (28%)] Loss: 0.558412\n",
      "Training set [3500/10745 (33%)] Loss: 0.519765\n",
      "Training set [4000/10745 (37%)] Loss: 0.463102\n",
      "Training set [4500/10745 (42%)] Loss: 0.521490\n",
      "Training set [5000/10745 (47%)] Loss: 0.562380\n",
      "Training set [5500/10745 (51%)] Loss: 0.591028\n",
      "Training set [6000/10745 (56%)] Loss: 0.563990\n",
      "Training set [6500/10745 (60%)] Loss: 0.597313\n",
      "Training set [7000/10745 (65%)] Loss: 0.419682\n",
      "Training set [7500/10745 (70%)] Loss: 0.597790\n",
      "Training set [8000/10745 (74%)] Loss: 0.515088\n",
      "Training set [8500/10745 (79%)] Loss: 0.600625\n",
      "Training set [9000/10745 (84%)] Loss: 0.427215\n",
      "Training set [9500/10745 (88%)] Loss: 0.539228\n",
      "Training set [10000/10745 (93%)] Loss: 0.544591\n",
      "Training set [10500/10745 (98%)] Loss: 0.552133\n",
      "Training set: Average loss: 0.541966\n",
      "Validation set: Average loss: 0.503455, Accuracy: 3733/4605 (81%\n",
      ")\n",
      "Epoch: 6\n",
      "Training set [0/10745 (0%)] Loss: 0.443256\n",
      "Training set [500/10745 (5%)] Loss: 0.380438\n",
      "Training set [1000/10745 (9%)] Loss: 0.465152\n",
      "Training set [1500/10745 (14%)] Loss: 0.491111\n",
      "Training set [2000/10745 (19%)] Loss: 0.502299\n",
      "Training set [2500/10745 (23%)] Loss: 0.375783\n",
      "Training set [3000/10745 (28%)] Loss: 0.495968\n",
      "Training set [3500/10745 (33%)] Loss: 0.456217\n",
      "Training set [4000/10745 (37%)] Loss: 0.394450\n",
      "Training set [4500/10745 (42%)] Loss: 0.465781\n",
      "Training set [5000/10745 (47%)] Loss: 0.501247\n",
      "Training set [5500/10745 (51%)] Loss: 0.546615\n",
      "Training set [6000/10745 (56%)] Loss: 0.548263\n",
      "Training set [6500/10745 (60%)] Loss: 0.530143\n",
      "Training set [7000/10745 (65%)] Loss: 0.359934\n",
      "Training set [7500/10745 (70%)] Loss: 0.557688\n",
      "Training set [8000/10745 (74%)] Loss: 0.477688\n",
      "Training set [8500/10745 (79%)] Loss: 0.582695\n",
      "Training set [9000/10745 (84%)] Loss: 0.368960\n",
      "Training set [9500/10745 (88%)] Loss: 0.497363\n",
      "Training set [10000/10745 (93%)] Loss: 0.502629\n",
      "Training set [10500/10745 (98%)] Loss: 0.516936\n",
      "Training set: Average loss: 0.483143\n",
      "Validation set: Average loss: 0.456937, Accuracy: 3802/4605 (83%\n",
      ")\n",
      "Epoch: 7\n",
      "Training set [0/10745 (0%)] Loss: 0.383485\n",
      "Training set [500/10745 (5%)] Loss: 0.350852\n",
      "Training set [1000/10745 (9%)] Loss: 0.427228\n",
      "Training set [1500/10745 (14%)] Loss: 0.444080\n",
      "Training set [2000/10745 (19%)] Loss: 0.467849\n",
      "Training set [2500/10745 (23%)] Loss: 0.315719\n",
      "Training set [3000/10745 (28%)] Loss: 0.427287\n",
      "Training set [3500/10745 (33%)] Loss: 0.415380\n",
      "Training set [4000/10745 (37%)] Loss: 0.354599\n",
      "Training set [4500/10745 (42%)] Loss: 0.422601\n",
      "Training set [5000/10745 (47%)] Loss: 0.459006\n",
      "Training set [5500/10745 (51%)] Loss: 0.506231\n",
      "Training set [6000/10745 (56%)] Loss: 0.508701\n",
      "Training set [6500/10745 (60%)] Loss: 0.492568\n",
      "Training set [7000/10745 (65%)] Loss: 0.315160\n",
      "Training set [7500/10745 (70%)] Loss: 0.548416\n",
      "Training set [8000/10745 (74%)] Loss: 0.412195\n",
      "Training set [8500/10745 (79%)] Loss: 0.535312\n",
      "Training set [9000/10745 (84%)] Loss: 0.320002\n",
      "Training set [9500/10745 (88%)] Loss: 0.477901\n",
      "Training set [10000/10745 (93%)] Loss: 0.450274\n",
      "Training set [10500/10745 (98%)] Loss: 0.469736\n",
      "Training set: Average loss: 0.441876\n",
      "Validation set: Average loss: 0.418558, Accuracy: 3871/4605 (84%\n",
      ")\n",
      "Epoch: 8\n",
      "Training set [0/10745 (0%)] Loss: 0.346451\n",
      "Training set [500/10745 (5%)] Loss: 0.317991\n",
      "Training set [1000/10745 (9%)] Loss: 0.393831\n",
      "Training set [1500/10745 (14%)] Loss: 0.408576\n",
      "Training set [2000/10745 (19%)] Loss: 0.400904\n",
      "Training set [2500/10745 (23%)] Loss: 0.295952\n",
      "Training set [3000/10745 (28%)] Loss: 0.383950\n",
      "Training set [3500/10745 (33%)] Loss: 0.370370\n",
      "Training set [4000/10745 (37%)] Loss: 0.308577\n",
      "Training set [4500/10745 (42%)] Loss: 0.399111\n",
      "Training set [5000/10745 (47%)] Loss: 0.431900\n",
      "Training set [5500/10745 (51%)] Loss: 0.444889\n",
      "Training set [6000/10745 (56%)] Loss: 0.499687\n",
      "Training set [6500/10745 (60%)] Loss: 0.448603\n",
      "Training set [7000/10745 (65%)] Loss: 0.299540\n",
      "Training set [7500/10745 (70%)] Loss: 0.531590\n",
      "Training set [8000/10745 (74%)] Loss: 0.401215\n",
      "Training set [8500/10745 (79%)] Loss: 0.529737\n",
      "Training set [9000/10745 (84%)] Loss: 0.303460\n",
      "Training set [9500/10745 (88%)] Loss: 0.439415\n",
      "Training set [10000/10745 (93%)] Loss: 0.412416\n",
      "Training set [10500/10745 (98%)] Loss: 0.424013\n",
      "Training set: Average loss: 0.408156\n",
      "Validation set: Average loss: 0.387312, Accuracy: 3923/4605 (85%\n",
      ")\n",
      "Epoch: 9\n",
      "Training set [0/10745 (0%)] Loss: 0.329452\n",
      "Training set [500/10745 (5%)] Loss: 0.288034\n",
      "Training set [1000/10745 (9%)] Loss: 0.356286\n",
      "Training set [1500/10745 (14%)] Loss: 0.368152\n",
      "Training set [2000/10745 (19%)] Loss: 0.382354\n",
      "Training set [2500/10745 (23%)] Loss: 0.268317\n",
      "Training set [3000/10745 (28%)] Loss: 0.368049\n",
      "Training set [3500/10745 (33%)] Loss: 0.349824\n",
      "Training set [4000/10745 (37%)] Loss: 0.285321\n",
      "Training set [4500/10745 (42%)] Loss: 0.388282\n",
      "Training set [5000/10745 (47%)] Loss: 0.403667\n",
      "Training set [5500/10745 (51%)] Loss: 0.412196\n",
      "Training set [6000/10745 (56%)] Loss: 0.479728\n",
      "Training set [6500/10745 (60%)] Loss: 0.421759\n",
      "Training set [7000/10745 (65%)] Loss: 0.295010\n",
      "Training set [7500/10745 (70%)] Loss: 0.504274\n",
      "Training set [8000/10745 (74%)] Loss: 0.357359\n",
      "Training set [8500/10745 (79%)] Loss: 0.501185\n",
      "Training set [9000/10745 (84%)] Loss: 0.282319\n",
      "Training set [9500/10745 (88%)] Loss: 0.420051\n",
      "Training set [10000/10745 (93%)] Loss: 0.370368\n",
      "Training set [10500/10745 (98%)] Loss: 0.411777\n",
      "Training set: Average loss: 0.382329\n",
      "Validation set: Average loss: 0.363818, Accuracy: 3970/4605 (86%\n",
      ")\n",
      "Epoch: 10\n",
      "Training set [0/10745 (0%)] Loss: 0.324539\n",
      "Training set [500/10745 (5%)] Loss: 0.276633\n",
      "Training set [1000/10745 (9%)] Loss: 0.319372\n",
      "Training set [1500/10745 (14%)] Loss: 0.341279\n",
      "Training set [2000/10745 (19%)] Loss: 0.348171\n",
      "Training set [2500/10745 (23%)] Loss: 0.245094\n",
      "Training set [3000/10745 (28%)] Loss: 0.326865\n",
      "Training set [3500/10745 (33%)] Loss: 0.312424\n",
      "Training set [4000/10745 (37%)] Loss: 0.267940\n",
      "Training set [4500/10745 (42%)] Loss: 0.356953\n",
      "Training set [5000/10745 (47%)] Loss: 0.372115\n",
      "Training set [5500/10745 (51%)] Loss: 0.383312\n",
      "Training set [6000/10745 (56%)] Loss: 0.464893\n",
      "Training set [6500/10745 (60%)] Loss: 0.413569\n",
      "Training set [7000/10745 (65%)] Loss: 0.272188\n",
      "Training set [7500/10745 (70%)] Loss: 0.495325\n",
      "Training set [8000/10745 (74%)] Loss: 0.327289\n",
      "Training set [8500/10745 (79%)] Loss: 0.479976\n",
      "Training set [9000/10745 (84%)] Loss: 0.264348\n",
      "Training set [9500/10745 (88%)] Loss: 0.387849\n",
      "Training set [10000/10745 (93%)] Loss: 0.346259\n",
      "Training set [10500/10745 (98%)] Loss: 0.413266\n",
      "Training set: Average loss: 0.359786\n",
      "Validation set: Average loss: 0.341924, Accuracy: 4018/4605 (87%\n",
      ")\n",
      "Epoch: 11\n",
      "Training set [0/10745 (0%)] Loss: 0.311382\n",
      "Training set [500/10745 (5%)] Loss: 0.262039\n",
      "Training set [1000/10745 (9%)] Loss: 0.294158\n",
      "Training set [1500/10745 (14%)] Loss: 0.338608\n",
      "Training set [2000/10745 (19%)] Loss: 0.331600\n",
      "Training set [2500/10745 (23%)] Loss: 0.231283\n",
      "Training set [3000/10745 (28%)] Loss: 0.308107\n",
      "Training set [3500/10745 (33%)] Loss: 0.334483\n",
      "Training set [4000/10745 (37%)] Loss: 0.258996\n",
      "Training set [4500/10745 (42%)] Loss: 0.337945\n",
      "Training set [5000/10745 (47%)] Loss: 0.368653\n",
      "Training set [5500/10745 (51%)] Loss: 0.348968\n",
      "Training set [6000/10745 (56%)] Loss: 0.470430\n",
      "Training set [6500/10745 (60%)] Loss: 0.383551\n",
      "Training set [7000/10745 (65%)] Loss: 0.278497\n",
      "Training set [7500/10745 (70%)] Loss: 0.470477\n",
      "Training set [8000/10745 (74%)] Loss: 0.311839\n",
      "Training set [8500/10745 (79%)] Loss: 0.446268\n",
      "Training set [9000/10745 (84%)] Loss: 0.247976\n",
      "Training set [9500/10745 (88%)] Loss: 0.372820\n",
      "Training set [10000/10745 (93%)] Loss: 0.325858\n",
      "Training set [10500/10745 (98%)] Loss: 0.389433\n",
      "Training set: Average loss: 0.343445\n",
      "Validation set: Average loss: 0.324708, Accuracy: 4056/4605 (88%\n",
      ")\n",
      "Epoch: 12\n",
      "Training set [0/10745 (0%)] Loss: 0.288013\n",
      "Training set [500/10745 (5%)] Loss: 0.256076\n",
      "Training set [1000/10745 (9%)] Loss: 0.276485\n",
      "Training set [1500/10745 (14%)] Loss: 0.308531\n",
      "Training set [2000/10745 (19%)] Loss: 0.283194\n",
      "Training set [2500/10745 (23%)] Loss: 0.216385\n",
      "Training set [3000/10745 (28%)] Loss: 0.284042\n",
      "Training set [3500/10745 (33%)] Loss: 0.313716\n",
      "Training set [4000/10745 (37%)] Loss: 0.236087\n",
      "Training set [4500/10745 (42%)] Loss: 0.342797\n",
      "Training set [5000/10745 (47%)] Loss: 0.348279\n",
      "Training set [5500/10745 (51%)] Loss: 0.345840\n",
      "Training set [6000/10745 (56%)] Loss: 0.438794\n",
      "Training set [6500/10745 (60%)] Loss: 0.364699\n",
      "Training set [7000/10745 (65%)] Loss: 0.268343\n",
      "Training set [7500/10745 (70%)] Loss: 0.469619\n",
      "Training set [8000/10745 (74%)] Loss: 0.299434\n",
      "Training set [8500/10745 (79%)] Loss: 0.440861\n",
      "Training set [9000/10745 (84%)] Loss: 0.250513\n",
      "Training set [9500/10745 (88%)] Loss: 0.352820\n",
      "Training set [10000/10745 (93%)] Loss: 0.306284\n",
      "Training set [10500/10745 (98%)] Loss: 0.387495\n",
      "Training set: Average loss: 0.329399\n",
      "Validation set: Average loss: 0.310577, Accuracy: 4083/4605 (89%\n",
      ")\n",
      "Epoch: 13\n",
      "Training set [0/10745 (0%)] Loss: 0.277752\n",
      "Training set [500/10745 (5%)] Loss: 0.252856\n",
      "Training set [1000/10745 (9%)] Loss: 0.263983\n",
      "Training set [1500/10745 (14%)] Loss: 0.300182\n",
      "Training set [2000/10745 (19%)] Loss: 0.277694\n",
      "Training set [2500/10745 (23%)] Loss: 0.195305\n",
      "Training set [3000/10745 (28%)] Loss: 0.273340\n",
      "Training set [3500/10745 (33%)] Loss: 0.320441\n",
      "Training set [4000/10745 (37%)] Loss: 0.232357\n",
      "Training set [4500/10745 (42%)] Loss: 0.310572\n",
      "Training set [5000/10745 (47%)] Loss: 0.320354\n",
      "Training set [5500/10745 (51%)] Loss: 0.313576\n",
      "Training set [6000/10745 (56%)] Loss: 0.443081\n",
      "Training set [6500/10745 (60%)] Loss: 0.352910\n",
      "Training set [7000/10745 (65%)] Loss: 0.270101\n",
      "Training set [7500/10745 (70%)] Loss: 0.472551\n",
      "Training set [8000/10745 (74%)] Loss: 0.272328\n",
      "Training set [8500/10745 (79%)] Loss: 0.441209\n",
      "Training set [9000/10745 (84%)] Loss: 0.222447\n",
      "Training set [9500/10745 (88%)] Loss: 0.322671\n",
      "Training set [10000/10745 (93%)] Loss: 0.292897\n",
      "Training set [10500/10745 (98%)] Loss: 0.379539\n",
      "Training set: Average loss: 0.317469\n",
      "Validation set: Average loss: 0.299101, Accuracy: 4102/4605 (89%\n",
      ")\n",
      "Epoch: 14\n",
      "Training set [0/10745 (0%)] Loss: 0.268766\n",
      "Training set [500/10745 (5%)] Loss: 0.237392\n",
      "Training set [1000/10745 (9%)] Loss: 0.258145\n",
      "Training set [1500/10745 (14%)] Loss: 0.277506\n",
      "Training set [2000/10745 (19%)] Loss: 0.265871\n",
      "Training set [2500/10745 (23%)] Loss: 0.188379\n",
      "Training set [3000/10745 (28%)] Loss: 0.244930\n",
      "Training set [3500/10745 (33%)] Loss: 0.326139\n",
      "Training set [4000/10745 (37%)] Loss: 0.214490\n",
      "Training set [4500/10745 (42%)] Loss: 0.299933\n",
      "Training set [5000/10745 (47%)] Loss: 0.316758\n",
      "Training set [5500/10745 (51%)] Loss: 0.316706\n",
      "Training set [6000/10745 (56%)] Loss: 0.429491\n",
      "Training set [6500/10745 (60%)] Loss: 0.348311\n",
      "Training set [7000/10745 (65%)] Loss: 0.256955\n",
      "Training set [7500/10745 (70%)] Loss: 0.441639\n",
      "Training set [8000/10745 (74%)] Loss: 0.266886\n",
      "Training set [8500/10745 (79%)] Loss: 0.409225\n",
      "Training set [9000/10745 (84%)] Loss: 0.223111\n",
      "Training set [9500/10745 (88%)] Loss: 0.320027\n",
      "Training set [10000/10745 (93%)] Loss: 0.293452\n",
      "Training set [10500/10745 (98%)] Loss: 0.366429\n",
      "Training set: Average loss: 0.306706\n",
      "Validation set: Average loss: 0.289313, Accuracy: 4123/4605 (90%\n",
      ")\n",
      "Epoch: 15\n",
      "Training set [0/10745 (0%)] Loss: 0.268966\n",
      "Training set [500/10745 (5%)] Loss: 0.222413\n",
      "Training set [1000/10745 (9%)] Loss: 0.244012\n",
      "Training set [1500/10745 (14%)] Loss: 0.278632\n",
      "Training set [2000/10745 (19%)] Loss: 0.260921\n",
      "Training set [2500/10745 (23%)] Loss: 0.189231\n",
      "Training set [3000/10745 (28%)] Loss: 0.242481\n",
      "Training set [3500/10745 (33%)] Loss: 0.303050\n",
      "Training set [4000/10745 (37%)] Loss: 0.201204\n",
      "Training set [4500/10745 (42%)] Loss: 0.278305\n",
      "Training set [5000/10745 (47%)] Loss: 0.299527\n",
      "Training set [5500/10745 (51%)] Loss: 0.311179\n",
      "Training set [6000/10745 (56%)] Loss: 0.421100\n",
      "Training set [6500/10745 (60%)] Loss: 0.328835\n",
      "Training set [7000/10745 (65%)] Loss: 0.257774\n",
      "Training set [7500/10745 (70%)] Loss: 0.464364\n",
      "Training set [8000/10745 (74%)] Loss: 0.247057\n",
      "Training set [8500/10745 (79%)] Loss: 0.410568\n",
      "Training set [9000/10745 (84%)] Loss: 0.209779\n",
      "Training set [9500/10745 (88%)] Loss: 0.306454\n",
      "Training set [10000/10745 (93%)] Loss: 0.286223\n",
      "Training set [10500/10745 (98%)] Loss: 0.353494\n",
      "Training set: Average loss: 0.298825\n",
      "Validation set: Average loss: 0.281071, Accuracy: 4140/4605 (90%\n",
      ")\n",
      "Epoch: 16\n",
      "Training set [0/10745 (0%)] Loss: 0.262466\n",
      "Training set [500/10745 (5%)] Loss: 0.234018\n",
      "Training set [1000/10745 (9%)] Loss: 0.239345\n",
      "Training set [1500/10745 (14%)] Loss: 0.259463\n",
      "Training set [2000/10745 (19%)] Loss: 0.229996\n",
      "Training set [2500/10745 (23%)] Loss: 0.188273\n",
      "Training set [3000/10745 (28%)] Loss: 0.247713\n",
      "Training set [3500/10745 (33%)] Loss: 0.310481\n",
      "Training set [4000/10745 (37%)] Loss: 0.188823\n",
      "Training set [4500/10745 (42%)] Loss: 0.268096\n",
      "Training set [5000/10745 (47%)] Loss: 0.294557\n",
      "Training set [5500/10745 (51%)] Loss: 0.296755\n",
      "Training set [6000/10745 (56%)] Loss: 0.416377\n",
      "Training set [6500/10745 (60%)] Loss: 0.347465\n",
      "Training set [7000/10745 (65%)] Loss: 0.265687\n",
      "Training set [7500/10745 (70%)] Loss: 0.436407\n",
      "Training set [8000/10745 (74%)] Loss: 0.245080\n",
      "Training set [8500/10745 (79%)] Loss: 0.389452\n",
      "Training set [9000/10745 (84%)] Loss: 0.204324\n",
      "Training set [9500/10745 (88%)] Loss: 0.302755\n",
      "Training set [10000/10745 (93%)] Loss: 0.275212\n",
      "Training set [10500/10745 (98%)] Loss: 0.362150\n",
      "Training set: Average loss: 0.291653\n",
      "Validation set: Average loss: 0.274390, Accuracy: 4157/4605 (90%\n",
      ")\n",
      "Epoch: 17\n",
      "Training set [0/10745 (0%)] Loss: 0.254674\n",
      "Training set [500/10745 (5%)] Loss: 0.223923\n",
      "Training set [1000/10745 (9%)] Loss: 0.231201\n",
      "Training set [1500/10745 (14%)] Loss: 0.257146\n",
      "Training set [2000/10745 (19%)] Loss: 0.244603\n",
      "Training set [2500/10745 (23%)] Loss: 0.174039\n",
      "Training set [3000/10745 (28%)] Loss: 0.217620\n",
      "Training set [3500/10745 (33%)] Loss: 0.288807\n",
      "Training set [4000/10745 (37%)] Loss: 0.192477\n",
      "Training set [4500/10745 (42%)] Loss: 0.258565\n",
      "Training set [5000/10745 (47%)] Loss: 0.280795\n",
      "Training set [5500/10745 (51%)] Loss: 0.270520\n",
      "Training set [6000/10745 (56%)] Loss: 0.417937\n",
      "Training set [6500/10745 (60%)] Loss: 0.318336\n",
      "Training set [7000/10745 (65%)] Loss: 0.242256\n",
      "Training set [7500/10745 (70%)] Loss: 0.432953\n",
      "Training set [8000/10745 (74%)] Loss: 0.228005\n",
      "Training set [8500/10745 (79%)] Loss: 0.385676\n",
      "Training set [9000/10745 (84%)] Loss: 0.199264\n",
      "Training set [9500/10745 (88%)] Loss: 0.271868\n",
      "Training set [10000/10745 (93%)] Loss: 0.268647\n",
      "Training set [10500/10745 (98%)] Loss: 0.374205\n",
      "Training set: Average loss: 0.284371\n",
      "Validation set: Average loss: 0.268935, Accuracy: 4167/4605 (90%\n",
      ")\n",
      "Epoch: 18\n",
      "Training set [0/10745 (0%)] Loss: 0.249993\n",
      "Training set [500/10745 (5%)] Loss: 0.220393\n",
      "Training set [1000/10745 (9%)] Loss: 0.229784\n",
      "Training set [1500/10745 (14%)] Loss: 0.252674\n",
      "Training set [2000/10745 (19%)] Loss: 0.215981\n",
      "Training set [2500/10745 (23%)] Loss: 0.163703\n",
      "Training set [3000/10745 (28%)] Loss: 0.223626\n",
      "Training set [3500/10745 (33%)] Loss: 0.307502\n",
      "Training set [4000/10745 (37%)] Loss: 0.186330\n",
      "Training set [4500/10745 (42%)] Loss: 0.237764\n",
      "Training set [5000/10745 (47%)] Loss: 0.269766\n",
      "Training set [5500/10745 (51%)] Loss: 0.274059\n",
      "Training set [6000/10745 (56%)] Loss: 0.398245\n",
      "Training set [6500/10745 (60%)] Loss: 0.308355\n",
      "Training set [7000/10745 (65%)] Loss: 0.254853\n",
      "Training set [7500/10745 (70%)] Loss: 0.446162\n",
      "Training set [8000/10745 (74%)] Loss: 0.216852\n",
      "Training set [8500/10745 (79%)] Loss: 0.380745\n",
      "Training set [9000/10745 (84%)] Loss: 0.187675\n",
      "Training set [9500/10745 (88%)] Loss: 0.277026\n",
      "Training set [10000/10745 (93%)] Loss: 0.263752\n",
      "Training set [10500/10745 (98%)] Loss: 0.353036\n",
      "Training set: Average loss: 0.279008\n",
      "Validation set: Average loss: 0.263325, Accuracy: 4193/4605 (91%\n",
      ")\n",
      "Epoch: 19\n",
      "Training set [0/10745 (0%)] Loss: 0.238361\n",
      "Training set [500/10745 (5%)] Loss: 0.217769\n",
      "Training set [1000/10745 (9%)] Loss: 0.214318\n",
      "Training set [1500/10745 (14%)] Loss: 0.235262\n",
      "Training set [2000/10745 (19%)] Loss: 0.209263\n",
      "Training set [2500/10745 (23%)] Loss: 0.161924\n",
      "Training set [3000/10745 (28%)] Loss: 0.226530\n",
      "Training set [3500/10745 (33%)] Loss: 0.309948\n",
      "Training set [4000/10745 (37%)] Loss: 0.172859\n",
      "Training set [4500/10745 (42%)] Loss: 0.239761\n",
      "Training set [5000/10745 (47%)] Loss: 0.248137\n",
      "Training set [5500/10745 (51%)] Loss: 0.272755\n",
      "Training set [6000/10745 (56%)] Loss: 0.407489\n",
      "Training set [6500/10745 (60%)] Loss: 0.316621\n",
      "Training set [7000/10745 (65%)] Loss: 0.241676\n",
      "Training set [7500/10745 (70%)] Loss: 0.437528\n",
      "Training set [8000/10745 (74%)] Loss: 0.213431\n",
      "Training set [8500/10745 (79%)] Loss: 0.366673\n",
      "Training set [9000/10745 (84%)] Loss: 0.176724\n",
      "Training set [9500/10745 (88%)] Loss: 0.274131\n",
      "Training set [10000/10745 (93%)] Loss: 0.267724\n",
      "Training set [10500/10745 (98%)] Loss: 0.366604\n",
      "Training set: Average loss: 0.273102\n",
      "Validation set: Average loss: 0.258884, Accuracy: 4202/4605 (91%\n",
      ")\n",
      "Epoch: 20\n",
      "Training set [0/10745 (0%)] Loss: 0.239352\n",
      "Training set [500/10745 (5%)] Loss: 0.208663\n",
      "Training set [1000/10745 (9%)] Loss: 0.216652\n",
      "Training set [1500/10745 (14%)] Loss: 0.235401\n",
      "Training set [2000/10745 (19%)] Loss: 0.210981\n",
      "Training set [2500/10745 (23%)] Loss: 0.147494\n",
      "Training set [3000/10745 (28%)] Loss: 0.214528\n",
      "Training set [3500/10745 (33%)] Loss: 0.314628\n",
      "Training set [4000/10745 (37%)] Loss: 0.174514\n",
      "Training set [4500/10745 (42%)] Loss: 0.222914\n",
      "Training set [5000/10745 (47%)] Loss: 0.249010\n",
      "Training set [5500/10745 (51%)] Loss: 0.273483\n",
      "Training set [6000/10745 (56%)] Loss: 0.380904\n",
      "Training set [6500/10745 (60%)] Loss: 0.306518\n",
      "Training set [7000/10745 (65%)] Loss: 0.235384\n",
      "Training set [7500/10745 (70%)] Loss: 0.442810\n",
      "Training set [8000/10745 (74%)] Loss: 0.202380\n",
      "Training set [8500/10745 (79%)] Loss: 0.376885\n",
      "Training set [9000/10745 (84%)] Loss: 0.178712\n",
      "Training set [9500/10745 (88%)] Loss: 0.289687\n",
      "Training set [10000/10745 (93%)] Loss: 0.254361\n",
      "Training set [10500/10745 (98%)] Loss: 0.337765\n",
      "Training set: Average loss: 0.268022\n",
      "Validation set: Average loss: 0.255671, Accuracy: 4197/4605 (91%\n",
      ")\n",
      "Epoch: 21\n",
      "Training set [0/10745 (0%)] Loss: 0.236234\n",
      "Training set [500/10745 (5%)] Loss: 0.212026\n",
      "Training set [1000/10745 (9%)] Loss: 0.202364\n",
      "Training set [1500/10745 (14%)] Loss: 0.228741\n",
      "Training set [2000/10745 (19%)] Loss: 0.217732\n",
      "Training set [2500/10745 (23%)] Loss: 0.149013\n",
      "Training set [3000/10745 (28%)] Loss: 0.214645\n",
      "Training set [3500/10745 (33%)] Loss: 0.303013\n",
      "Training set [4000/10745 (37%)] Loss: 0.149740\n",
      "Training set [4500/10745 (42%)] Loss: 0.220115\n",
      "Training set [5000/10745 (47%)] Loss: 0.237040\n",
      "Training set [5500/10745 (51%)] Loss: 0.268279\n",
      "Training set [6000/10745 (56%)] Loss: 0.393024\n",
      "Training set [6500/10745 (60%)] Loss: 0.288788\n",
      "Training set [7000/10745 (65%)] Loss: 0.232328\n",
      "Training set [7500/10745 (70%)] Loss: 0.423537\n",
      "Training set [8000/10745 (74%)] Loss: 0.206833\n",
      "Training set [8500/10745 (79%)] Loss: 0.350980\n",
      "Training set [9000/10745 (84%)] Loss: 0.175816\n",
      "Training set [9500/10745 (88%)] Loss: 0.259633\n",
      "Training set [10000/10745 (93%)] Loss: 0.256793\n",
      "Training set [10500/10745 (98%)] Loss: 0.347243\n",
      "Training set: Average loss: 0.263757\n",
      "Validation set: Average loss: 0.251046, Accuracy: 4208/4605 (91%\n",
      ")\n",
      "Epoch: 22\n",
      "Training set [0/10745 (0%)] Loss: 0.232014\n",
      "Training set [500/10745 (5%)] Loss: 0.202445\n",
      "Training set [1000/10745 (9%)] Loss: 0.200163\n",
      "Training set [1500/10745 (14%)] Loss: 0.211269\n",
      "Training set [2000/10745 (19%)] Loss: 0.206037\n",
      "Training set [2500/10745 (23%)] Loss: 0.146288\n",
      "Training set [3000/10745 (28%)] Loss: 0.201790\n",
      "Training set [3500/10745 (33%)] Loss: 0.300556\n",
      "Training set [4000/10745 (37%)] Loss: 0.157279\n",
      "Training set [4500/10745 (42%)] Loss: 0.201042\n",
      "Training set [5000/10745 (47%)] Loss: 0.235993\n",
      "Training set [5500/10745 (51%)] Loss: 0.263247\n",
      "Training set [6000/10745 (56%)] Loss: 0.409506\n",
      "Training set [6500/10745 (60%)] Loss: 0.283685\n",
      "Training set [7000/10745 (65%)] Loss: 0.225118\n",
      "Training set [7500/10745 (70%)] Loss: 0.409266\n",
      "Training set [8000/10745 (74%)] Loss: 0.212112\n",
      "Training set [8500/10745 (79%)] Loss: 0.344666\n",
      "Training set [9000/10745 (84%)] Loss: 0.177268\n",
      "Training set [9500/10745 (88%)] Loss: 0.265431\n",
      "Training set [10000/10745 (93%)] Loss: 0.269250\n",
      "Training set [10500/10745 (98%)] Loss: 0.338470\n",
      "Training set: Average loss: 0.259728\n",
      "Validation set: Average loss: 0.247371, Accuracy: 4214/4605 (92%\n",
      ")\n",
      "Epoch: 23\n",
      "Training set [0/10745 (0%)] Loss: 0.224467\n",
      "Training set [500/10745 (5%)] Loss: 0.205207\n",
      "Training set [1000/10745 (9%)] Loss: 0.198551\n",
      "Training set [1500/10745 (14%)] Loss: 0.197503\n",
      "Training set [2000/10745 (19%)] Loss: 0.198263\n",
      "Training set [2500/10745 (23%)] Loss: 0.145429\n",
      "Training set [3000/10745 (28%)] Loss: 0.220565\n",
      "Training set [3500/10745 (33%)] Loss: 0.298927\n",
      "Training set [4000/10745 (37%)] Loss: 0.150466\n",
      "Training set [4500/10745 (42%)] Loss: 0.213468\n",
      "Training set [5000/10745 (47%)] Loss: 0.209104\n",
      "Training set [5500/10745 (51%)] Loss: 0.257795\n",
      "Training set [6000/10745 (56%)] Loss: 0.373900\n",
      "Training set [6500/10745 (60%)] Loss: 0.289915\n",
      "Training set [7000/10745 (65%)] Loss: 0.231242\n",
      "Training set [7500/10745 (70%)] Loss: 0.412013\n",
      "Training set [8000/10745 (74%)] Loss: 0.187785\n",
      "Training set [8500/10745 (79%)] Loss: 0.343302\n",
      "Training set [9000/10745 (84%)] Loss: 0.174669\n",
      "Training set [9500/10745 (88%)] Loss: 0.287894\n",
      "Training set [10000/10745 (93%)] Loss: 0.258764\n",
      "Training set [10500/10745 (98%)] Loss: 0.350513\n",
      "Training set: Average loss: 0.256118\n",
      "Validation set: Average loss: 0.244719, Accuracy: 4222/4605 (92%\n",
      ")\n",
      "Epoch: 24\n",
      "Training set [0/10745 (0%)] Loss: 0.237454\n",
      "Training set [500/10745 (5%)] Loss: 0.210914\n",
      "Training set [1000/10745 (9%)] Loss: 0.204453\n",
      "Training set [1500/10745 (14%)] Loss: 0.197131\n",
      "Training set [2000/10745 (19%)] Loss: 0.193206\n",
      "Training set [2500/10745 (23%)] Loss: 0.124371\n",
      "Training set [3000/10745 (28%)] Loss: 0.203085\n",
      "Training set [3500/10745 (33%)] Loss: 0.290892\n",
      "Training set [4000/10745 (37%)] Loss: 0.155092\n",
      "Training set [4500/10745 (42%)] Loss: 0.201281\n",
      "Training set [5000/10745 (47%)] Loss: 0.239034\n",
      "Training set [5500/10745 (51%)] Loss: 0.243734\n",
      "Training set [6000/10745 (56%)] Loss: 0.383598\n",
      "Training set [6500/10745 (60%)] Loss: 0.281629\n",
      "Training set [7000/10745 (65%)] Loss: 0.245963\n",
      "Training set [7500/10745 (70%)] Loss: 0.422182\n",
      "Training set [8000/10745 (74%)] Loss: 0.203351\n",
      "Training set [8500/10745 (79%)] Loss: 0.332860\n",
      "Training set [9000/10745 (84%)] Loss: 0.172051\n",
      "Training set [9500/10745 (88%)] Loss: 0.250169\n",
      "Training set [10000/10745 (93%)] Loss: 0.246707\n",
      "Training set [10500/10745 (98%)] Loss: 0.347899\n",
      "Training set: Average loss: 0.252629\n",
      "Validation set: Average loss: 0.242243, Accuracy: 4228/4605 (92%\n",
      ")\n",
      "Epoch: 25\n",
      "Training set [0/10745 (0%)] Loss: 0.226210\n",
      "Training set [500/10745 (5%)] Loss: 0.210846\n",
      "Training set [1000/10745 (9%)] Loss: 0.208158\n",
      "Training set [1500/10745 (14%)] Loss: 0.213008\n",
      "Training set [2000/10745 (19%)] Loss: 0.190780\n",
      "Training set [2500/10745 (23%)] Loss: 0.136574\n",
      "Training set [3000/10745 (28%)] Loss: 0.196814\n",
      "Training set [3500/10745 (33%)] Loss: 0.283779\n",
      "Training set [4000/10745 (37%)] Loss: 0.145268\n",
      "Training set [4500/10745 (42%)] Loss: 0.209380\n",
      "Training set [5000/10745 (47%)] Loss: 0.222533\n",
      "Training set [5500/10745 (51%)] Loss: 0.248631\n",
      "Training set [6000/10745 (56%)] Loss: 0.391842\n",
      "Training set [6500/10745 (60%)] Loss: 0.288963\n",
      "Training set [7000/10745 (65%)] Loss: 0.239243\n",
      "Training set [7500/10745 (70%)] Loss: 0.389091\n",
      "Training set [8000/10745 (74%)] Loss: 0.191949\n",
      "Training set [8500/10745 (79%)] Loss: 0.328984\n",
      "Training set [9000/10745 (84%)] Loss: 0.172247\n",
      "Training set [9500/10745 (88%)] Loss: 0.265565\n",
      "Training set [10000/10745 (93%)] Loss: 0.239871\n",
      "Training set [10500/10745 (98%)] Loss: 0.313886\n",
      "Training set: Average loss: 0.248854\n",
      "Validation set: Average loss: 0.238555, Accuracy: 4226/4605 (92%\n",
      ")\n",
      "Epoch: 26\n",
      "Training set [0/10745 (0%)] Loss: 0.225596\n",
      "Training set [500/10745 (5%)] Loss: 0.192048\n",
      "Training set [1000/10745 (9%)] Loss: 0.194071\n",
      "Training set [1500/10745 (14%)] Loss: 0.222339\n",
      "Training set [2000/10745 (19%)] Loss: 0.189810\n",
      "Training set [2500/10745 (23%)] Loss: 0.117429\n",
      "Training set [3000/10745 (28%)] Loss: 0.195812\n",
      "Training set [3500/10745 (33%)] Loss: 0.276610\n",
      "Training set [4000/10745 (37%)] Loss: 0.164504\n",
      "Training set [4500/10745 (42%)] Loss: 0.187775\n",
      "Training set [5000/10745 (47%)] Loss: 0.219307\n",
      "Training set [5500/10745 (51%)] Loss: 0.254725\n",
      "Training set [6000/10745 (56%)] Loss: 0.388179\n",
      "Training set [6500/10745 (60%)] Loss: 0.273056\n",
      "Training set [7000/10745 (65%)] Loss: 0.237721\n",
      "Training set [7500/10745 (70%)] Loss: 0.383467\n",
      "Training set [8000/10745 (74%)] Loss: 0.193612\n",
      "Training set [8500/10745 (79%)] Loss: 0.349569\n",
      "Training set [9000/10745 (84%)] Loss: 0.174015\n",
      "Training set [9500/10745 (88%)] Loss: 0.268834\n",
      "Training set [10000/10745 (93%)] Loss: 0.260830\n",
      "Training set [10500/10745 (98%)] Loss: 0.329669\n",
      "Training set: Average loss: 0.245822\n",
      "Validation set: Average loss: 0.235798, Accuracy: 4232/4605 (92%\n",
      ")\n",
      "Epoch: 27\n",
      "Training set [0/10745 (0%)] Loss: 0.223719\n",
      "Training set [500/10745 (5%)] Loss: 0.193789\n",
      "Training set [1000/10745 (9%)] Loss: 0.199192\n",
      "Training set [1500/10745 (14%)] Loss: 0.199943\n",
      "Training set [2000/10745 (19%)] Loss: 0.174822\n",
      "Training set [2500/10745 (23%)] Loss: 0.118898\n",
      "Training set [3000/10745 (28%)] Loss: 0.180325\n",
      "Training set [3500/10745 (33%)] Loss: 0.284717\n",
      "Training set [4000/10745 (37%)] Loss: 0.148465\n",
      "Training set [4500/10745 (42%)] Loss: 0.177489\n",
      "Training set [5000/10745 (47%)] Loss: 0.221082\n",
      "Training set [5500/10745 (51%)] Loss: 0.231793\n",
      "Training set [6000/10745 (56%)] Loss: 0.372749\n",
      "Training set [6500/10745 (60%)] Loss: 0.271343\n",
      "Training set [7000/10745 (65%)] Loss: 0.237433\n",
      "Training set [7500/10745 (70%)] Loss: 0.411312\n",
      "Training set [8000/10745 (74%)] Loss: 0.179313\n",
      "Training set [8500/10745 (79%)] Loss: 0.345977\n",
      "Training set [9000/10745 (84%)] Loss: 0.162039\n",
      "Training set [9500/10745 (88%)] Loss: 0.272938\n",
      "Training set [10000/10745 (93%)] Loss: 0.225380\n",
      "Training set [10500/10745 (98%)] Loss: 0.316216\n",
      "Training set: Average loss: 0.242689\n",
      "Validation set: Average loss: 0.233178, Accuracy: 4238/4605 (92%\n",
      ")\n",
      "Epoch: 28\n",
      "Training set [0/10745 (0%)] Loss: 0.206596\n",
      "Training set [500/10745 (5%)] Loss: 0.195145\n",
      "Training set [1000/10745 (9%)] Loss: 0.204106\n",
      "Training set [1500/10745 (14%)] Loss: 0.200220\n",
      "Training set [2000/10745 (19%)] Loss: 0.175542\n",
      "Training set [2500/10745 (23%)] Loss: 0.113289\n",
      "Training set [3000/10745 (28%)] Loss: 0.182084\n",
      "Training set [3500/10745 (33%)] Loss: 0.288707\n",
      "Training set [4000/10745 (37%)] Loss: 0.136103\n",
      "Training set [4500/10745 (42%)] Loss: 0.195470\n",
      "Training set [5000/10745 (47%)] Loss: 0.196702\n",
      "Training set [5500/10745 (51%)] Loss: 0.238875\n",
      "Training set [6000/10745 (56%)] Loss: 0.356562\n",
      "Training set [6500/10745 (60%)] Loss: 0.252812\n",
      "Training set [7000/10745 (65%)] Loss: 0.219589\n",
      "Training set [7500/10745 (70%)] Loss: 0.400728\n",
      "Training set [8000/10745 (74%)] Loss: 0.155992\n",
      "Training set [8500/10745 (79%)] Loss: 0.314691\n",
      "Training set [9000/10745 (84%)] Loss: 0.178076\n",
      "Training set [9500/10745 (88%)] Loss: 0.257513\n",
      "Training set [10000/10745 (93%)] Loss: 0.241734\n",
      "Training set [10500/10745 (98%)] Loss: 0.319217\n",
      "Training set: Average loss: 0.238750\n",
      "Validation set: Average loss: 0.230697, Accuracy: 4247/4605 (92%\n",
      ")\n",
      "Epoch: 29\n",
      "Training set [0/10745 (0%)] Loss: 0.212704\n",
      "Training set [500/10745 (5%)] Loss: 0.198899\n",
      "Training set [1000/10745 (9%)] Loss: 0.183905\n",
      "Training set [1500/10745 (14%)] Loss: 0.188373\n",
      "Training set [2000/10745 (19%)] Loss: 0.165845\n",
      "Training set [2500/10745 (23%)] Loss: 0.120898\n",
      "Training set [3000/10745 (28%)] Loss: 0.193625\n",
      "Training set [3500/10745 (33%)] Loss: 0.261425\n",
      "Training set [4000/10745 (37%)] Loss: 0.132905\n",
      "Training set [4500/10745 (42%)] Loss: 0.196374\n",
      "Training set [5000/10745 (47%)] Loss: 0.219586\n",
      "Training set [5500/10745 (51%)] Loss: 0.223447\n",
      "Training set [6000/10745 (56%)] Loss: 0.372482\n",
      "Training set [6500/10745 (60%)] Loss: 0.271481\n",
      "Training set [7000/10745 (65%)] Loss: 0.215374\n",
      "Training set [7500/10745 (70%)] Loss: 0.372808\n",
      "Training set [8000/10745 (74%)] Loss: 0.161659\n",
      "Training set [8500/10745 (79%)] Loss: 0.314409\n",
      "Training set [9000/10745 (84%)] Loss: 0.168364\n",
      "Training set [9500/10745 (88%)] Loss: 0.254451\n",
      "Training set [10000/10745 (93%)] Loss: 0.225314\n",
      "Training set [10500/10745 (98%)] Loss: 0.314705\n",
      "Training set: Average loss: 0.235500\n",
      "Validation set: Average loss: 0.228879, Accuracy: 4260/4605 (93%\n",
      ")\n",
      "Epoch: 30\n",
      "Training set [0/10745 (0%)] Loss: 0.200984\n",
      "Training set [500/10745 (5%)] Loss: 0.198305\n",
      "Training set [1000/10745 (9%)] Loss: 0.191123\n",
      "Training set [1500/10745 (14%)] Loss: 0.185102\n",
      "Training set [2000/10745 (19%)] Loss: 0.167305\n",
      "Training set [2500/10745 (23%)] Loss: 0.110887\n",
      "Training set [3000/10745 (28%)] Loss: 0.180648\n",
      "Training set [3500/10745 (33%)] Loss: 0.275731\n",
      "Training set [4000/10745 (37%)] Loss: 0.134499\n",
      "Training set [4500/10745 (42%)] Loss: 0.191367\n",
      "Training set [5000/10745 (47%)] Loss: 0.203068\n",
      "Training set [5500/10745 (51%)] Loss: 0.233956\n",
      "Training set [6000/10745 (56%)] Loss: 0.340734\n",
      "Training set [6500/10745 (60%)] Loss: 0.249204\n",
      "Training set [7000/10745 (65%)] Loss: 0.218739\n",
      "Training set [7500/10745 (70%)] Loss: 0.367370\n",
      "Training set [8000/10745 (74%)] Loss: 0.168905\n",
      "Training set [8500/10745 (79%)] Loss: 0.320897\n",
      "Training set [9000/10745 (84%)] Loss: 0.159841\n",
      "Training set [9500/10745 (88%)] Loss: 0.255271\n",
      "Training set [10000/10745 (93%)] Loss: 0.232829\n",
      "Training set [10500/10745 (98%)] Loss: 0.314565\n",
      "Training set: Average loss: 0.233131\n",
      "Validation set: Average loss: 0.225474, Accuracy: 4248/4605 (92%\n",
      ")\n",
      "Epoch: 31\n",
      "Training set [0/10745 (0%)] Loss: 0.191260\n",
      "Training set [500/10745 (5%)] Loss: 0.197063\n",
      "Training set [1000/10745 (9%)] Loss: 0.195149\n",
      "Training set [1500/10745 (14%)] Loss: 0.161769\n",
      "Training set [2000/10745 (19%)] Loss: 0.170631\n",
      "Training set [2500/10745 (23%)] Loss: 0.112790\n",
      "Training set [3000/10745 (28%)] Loss: 0.179047\n",
      "Training set [3500/10745 (33%)] Loss: 0.281783\n",
      "Training set [4000/10745 (37%)] Loss: 0.144988\n",
      "Training set [4500/10745 (42%)] Loss: 0.186584\n",
      "Training set [5000/10745 (47%)] Loss: 0.200565\n",
      "Training set [5500/10745 (51%)] Loss: 0.239142\n",
      "Training set [6000/10745 (56%)] Loss: 0.351610\n",
      "Training set [6500/10745 (60%)] Loss: 0.250186\n",
      "Training set [7000/10745 (65%)] Loss: 0.240975\n",
      "Training set [7500/10745 (70%)] Loss: 0.349669\n",
      "Training set [8000/10745 (74%)] Loss: 0.189100\n",
      "Training set [8500/10745 (79%)] Loss: 0.320393\n",
      "Training set [9000/10745 (84%)] Loss: 0.158923\n",
      "Training set [9500/10745 (88%)] Loss: 0.248133\n",
      "Training set [10000/10745 (93%)] Loss: 0.221015\n",
      "Training set [10500/10745 (98%)] Loss: 0.312188\n",
      "Training set: Average loss: 0.231086\n",
      "Validation set: Average loss: 0.223263, Accuracy: 4252/4605 (92%\n",
      ")\n",
      "Epoch: 32\n",
      "Training set [0/10745 (0%)] Loss: 0.194617\n",
      "Training set [500/10745 (5%)] Loss: 0.207774\n",
      "Training set [1000/10745 (9%)] Loss: 0.194250\n",
      "Training set [1500/10745 (14%)] Loss: 0.169031\n",
      "Training set [2000/10745 (19%)] Loss: 0.171394\n",
      "Training set [2500/10745 (23%)] Loss: 0.107075\n",
      "Training set [3000/10745 (28%)] Loss: 0.175368\n",
      "Training set [3500/10745 (33%)] Loss: 0.281623\n",
      "Training set [4000/10745 (37%)] Loss: 0.134930\n",
      "Training set [4500/10745 (42%)] Loss: 0.164790\n",
      "Training set [5000/10745 (47%)] Loss: 0.205407\n",
      "Training set [5500/10745 (51%)] Loss: 0.242954\n",
      "Training set [6000/10745 (56%)] Loss: 0.360578\n",
      "Training set [6500/10745 (60%)] Loss: 0.255907\n",
      "Training set [7000/10745 (65%)] Loss: 0.234655\n",
      "Training set [7500/10745 (70%)] Loss: 0.343172\n",
      "Training set [8000/10745 (74%)] Loss: 0.159182\n",
      "Training set [8500/10745 (79%)] Loss: 0.310085\n",
      "Training set [9000/10745 (84%)] Loss: 0.165596\n",
      "Training set [9500/10745 (88%)] Loss: 0.262889\n",
      "Training set [10000/10745 (93%)] Loss: 0.216321\n",
      "Training set [10500/10745 (98%)] Loss: 0.310111\n",
      "Training set: Average loss: 0.228205\n",
      "Validation set: Average loss: 0.221818, Accuracy: 4266/4605 (93%\n",
      ")\n",
      "Epoch: 33\n",
      "Training set [0/10745 (0%)] Loss: 0.214987\n",
      "Training set [500/10745 (5%)] Loss: 0.177063\n",
      "Training set [1000/10745 (9%)] Loss: 0.185327\n",
      "Training set [1500/10745 (14%)] Loss: 0.175552\n",
      "Training set [2000/10745 (19%)] Loss: 0.157647\n",
      "Training set [2500/10745 (23%)] Loss: 0.111086\n",
      "Training set [3000/10745 (28%)] Loss: 0.178879\n",
      "Training set [3500/10745 (33%)] Loss: 0.265746\n",
      "Training set [4000/10745 (37%)] Loss: 0.141478\n",
      "Training set [4500/10745 (42%)] Loss: 0.198785\n",
      "Training set [5000/10745 (47%)] Loss: 0.199411\n",
      "Training set [5500/10745 (51%)] Loss: 0.231746\n",
      "Training set [6000/10745 (56%)] Loss: 0.341991\n",
      "Training set [6500/10745 (60%)] Loss: 0.242484\n",
      "Training set [7000/10745 (65%)] Loss: 0.207924\n",
      "Training set [7500/10745 (70%)] Loss: 0.373034\n",
      "Training set [8000/10745 (74%)] Loss: 0.154308\n",
      "Training set [8500/10745 (79%)] Loss: 0.328519\n",
      "Training set [9000/10745 (84%)] Loss: 0.155719\n",
      "Training set [9500/10745 (88%)] Loss: 0.264515\n",
      "Training set [10000/10745 (93%)] Loss: 0.244225\n",
      "Training set [10500/10745 (98%)] Loss: 0.314760\n",
      "Training set: Average loss: 0.227203\n",
      "Validation set: Average loss: 0.219914, Accuracy: 4271/4605 (93%\n",
      ")\n",
      "Epoch: 34\n",
      "Training set [0/10745 (0%)] Loss: 0.200957\n",
      "Training set [500/10745 (5%)] Loss: 0.204915\n",
      "Training set [1000/10745 (9%)] Loss: 0.186818\n",
      "Training set [1500/10745 (14%)] Loss: 0.164584\n",
      "Training set [2000/10745 (19%)] Loss: 0.156455\n",
      "Training set [2500/10745 (23%)] Loss: 0.092040\n",
      "Training set [3000/10745 (28%)] Loss: 0.177738\n",
      "Training set [3500/10745 (33%)] Loss: 0.270825\n",
      "Training set [4000/10745 (37%)] Loss: 0.125390\n",
      "Training set [4500/10745 (42%)] Loss: 0.189308\n",
      "Training set [5000/10745 (47%)] Loss: 0.196485\n",
      "Training set [5500/10745 (51%)] Loss: 0.228977\n",
      "Training set [6000/10745 (56%)] Loss: 0.342305\n",
      "Training set [6500/10745 (60%)] Loss: 0.261510\n",
      "Training set [7000/10745 (65%)] Loss: 0.219594\n",
      "Training set [7500/10745 (70%)] Loss: 0.360235\n",
      "Training set [8000/10745 (74%)] Loss: 0.158796\n",
      "Training set [8500/10745 (79%)] Loss: 0.304879\n",
      "Training set [9000/10745 (84%)] Loss: 0.153955\n",
      "Training set [9500/10745 (88%)] Loss: 0.252734\n",
      "Training set [10000/10745 (93%)] Loss: 0.220981\n",
      "Training set [10500/10745 (98%)] Loss: 0.295732\n",
      "Training set: Average loss: 0.223406\n",
      "Validation set: Average loss: 0.217289, Accuracy: 4274/4605 (93%\n",
      ")\n",
      "Epoch: 35\n",
      "Training set [0/10745 (0%)] Loss: 0.182597\n",
      "Training set [500/10745 (5%)] Loss: 0.185290\n",
      "Training set [1000/10745 (9%)] Loss: 0.184359\n",
      "Training set [1500/10745 (14%)] Loss: 0.163991\n",
      "Training set [2000/10745 (19%)] Loss: 0.150171\n",
      "Training set [2500/10745 (23%)] Loss: 0.098495\n",
      "Training set [3000/10745 (28%)] Loss: 0.172163\n",
      "Training set [3500/10745 (33%)] Loss: 0.244886\n",
      "Training set [4000/10745 (37%)] Loss: 0.143699\n",
      "Training set [4500/10745 (42%)] Loss: 0.181324\n",
      "Training set [5000/10745 (47%)] Loss: 0.187962\n",
      "Training set [5500/10745 (51%)] Loss: 0.231833\n",
      "Training set [6000/10745 (56%)] Loss: 0.352671\n",
      "Training set [6500/10745 (60%)] Loss: 0.224004\n",
      "Training set [7000/10745 (65%)] Loss: 0.227301\n",
      "Training set [7500/10745 (70%)] Loss: 0.355128\n",
      "Training set [8000/10745 (74%)] Loss: 0.156398\n",
      "Training set [8500/10745 (79%)] Loss: 0.294446\n",
      "Training set [9000/10745 (84%)] Loss: 0.165672\n",
      "Training set [9500/10745 (88%)] Loss: 0.249841\n",
      "Training set [10000/10745 (93%)] Loss: 0.224619\n",
      "Training set [10500/10745 (98%)] Loss: 0.293513\n",
      "Training set: Average loss: 0.221282\n",
      "Validation set: Average loss: 0.215681, Accuracy: 4280/4605 (93%\n",
      ")\n",
      "Epoch: 36\n",
      "Training set [0/10745 (0%)] Loss: 0.177952\n",
      "Training set [500/10745 (5%)] Loss: 0.186613\n",
      "Training set [1000/10745 (9%)] Loss: 0.186426\n",
      "Training set [1500/10745 (14%)] Loss: 0.172923\n",
      "Training set [2000/10745 (19%)] Loss: 0.155006\n",
      "Training set [2500/10745 (23%)] Loss: 0.095143\n",
      "Training set [3000/10745 (28%)] Loss: 0.182361\n",
      "Training set [3500/10745 (33%)] Loss: 0.262206\n",
      "Training set [4000/10745 (37%)] Loss: 0.135897\n",
      "Training set [4500/10745 (42%)] Loss: 0.188230\n",
      "Training set [5000/10745 (47%)] Loss: 0.183781\n",
      "Training set [5500/10745 (51%)] Loss: 0.221324\n",
      "Training set [6000/10745 (56%)] Loss: 0.318818\n",
      "Training set [6500/10745 (60%)] Loss: 0.235241\n",
      "Training set [7000/10745 (65%)] Loss: 0.206660\n",
      "Training set [7500/10745 (70%)] Loss: 0.344332\n",
      "Training set [8000/10745 (74%)] Loss: 0.144733\n",
      "Training set [8500/10745 (79%)] Loss: 0.275786\n",
      "Training set [9000/10745 (84%)] Loss: 0.169120\n",
      "Training set [9500/10745 (88%)] Loss: 0.275509\n",
      "Training set [10000/10745 (93%)] Loss: 0.234204\n",
      "Training set [10500/10745 (98%)] Loss: 0.294979\n",
      "Training set: Average loss: 0.220261\n",
      "Validation set: Average loss: 0.214136, Accuracy: 4286/4605 (93%\n",
      ")\n",
      "Epoch: 37\n",
      "Training set [0/10745 (0%)] Loss: 0.187386\n",
      "Training set [500/10745 (5%)] Loss: 0.190967\n",
      "Training set [1000/10745 (9%)] Loss: 0.182553\n",
      "Training set [1500/10745 (14%)] Loss: 0.156002\n",
      "Training set [2000/10745 (19%)] Loss: 0.141353\n",
      "Training set [2500/10745 (23%)] Loss: 0.108762\n",
      "Training set [3000/10745 (28%)] Loss: 0.177347\n",
      "Training set [3500/10745 (33%)] Loss: 0.270771\n",
      "Training set [4000/10745 (37%)] Loss: 0.117400\n",
      "Training set [4500/10745 (42%)] Loss: 0.176378\n",
      "Training set [5000/10745 (47%)] Loss: 0.171679\n",
      "Training set [5500/10745 (51%)] Loss: 0.218850\n",
      "Training set [6000/10745 (56%)] Loss: 0.322896\n",
      "Training set [6500/10745 (60%)] Loss: 0.256903\n",
      "Training set [7000/10745 (65%)] Loss: 0.211638\n",
      "Training set [7500/10745 (70%)] Loss: 0.345969\n",
      "Training set [8000/10745 (74%)] Loss: 0.153888\n",
      "Training set [8500/10745 (79%)] Loss: 0.305324\n",
      "Training set [9000/10745 (84%)] Loss: 0.160170\n",
      "Training set [9500/10745 (88%)] Loss: 0.268186\n",
      "Training set [10000/10745 (93%)] Loss: 0.229465\n",
      "Training set [10500/10745 (98%)] Loss: 0.289975\n",
      "Training set: Average loss: 0.216551\n",
      "Validation set: Average loss: 0.211478, Accuracy: 4284/4605 (93%\n",
      ")\n",
      "Epoch: 38\n",
      "Training set [0/10745 (0%)] Loss: 0.194694\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 25\u001B[0m\n\u001B[0;32m     23\u001B[0m epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m70\u001B[39m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, epochs \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m---> 25\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     26\u001B[0m     test_loss \u001B[38;5;241m=\u001B[39m test(model, device, test_loader)\n\u001B[0;32m     27\u001B[0m     epoch_nums\u001B[38;5;241m.\u001B[39mappend(epoch)\n",
      "Cell \u001B[1;32mIn[4], line 7\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, device, train_loader, optimizer, epoch)\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Process the images in batches\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch_idx, (data, target) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_loader):\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;66;03m# Use the CPU or GPU as appropriate\u001B[39;00m\n\u001B[0;32m      9\u001B[0m     data, target \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mto(device), target\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;66;03m# Reset the optimizer\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\python_swe\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    631\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    632\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 633\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    634\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    635\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    636\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    637\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\python_swe\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    675\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    676\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 677\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    678\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    679\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\python_swe\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpossibly_batched_index\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\python_swe\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\python_swe\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:298\u001B[0m, in \u001B[0;36mSubset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m    296\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(idx, \u001B[38;5;28mlist\u001B[39m):\n\u001B[0;32m    297\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m idx]]\n\u001B[1;32m--> 298\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\python_swe\\Lib\\site-packages\\torchvision\\datasets\\folder.py:229\u001B[0m, in \u001B[0;36mDatasetFolder.__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m    221\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    222\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m    223\u001B[0m \u001B[38;5;124;03m    index (int): Index\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    226\u001B[0m \u001B[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001B[39;00m\n\u001B[0;32m    227\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    228\u001B[0m path, target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msamples[index]\n\u001B[1;32m--> 229\u001B[0m sample \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    230\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    231\u001B[0m     sample \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform(sample)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\python_swe\\Lib\\site-packages\\torchvision\\datasets\\folder.py:268\u001B[0m, in \u001B[0;36mdefault_loader\u001B[1;34m(path)\u001B[0m\n\u001B[0;32m    266\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m accimage_loader(path)\n\u001B[0;32m    267\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 268\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpil_loader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\python_swe\\Lib\\site-packages\\torchvision\\datasets\\folder.py:248\u001B[0m, in \u001B[0;36mpil_loader\u001B[1;34m(path)\u001B[0m\n\u001B[0;32m    246\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m    247\u001B[0m     img \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mopen(f)\n\u001B[1;32m--> 248\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mRGB\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\python_swe\\Lib\\site-packages\\PIL\\Image.py:921\u001B[0m, in \u001B[0;36mImage.convert\u001B[1;34m(self, mode, matrix, dither, palette, colors)\u001B[0m\n\u001B[0;32m    873\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconvert\u001B[39m(\n\u001B[0;32m    874\u001B[0m     \u001B[38;5;28mself\u001B[39m, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, matrix\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, dither\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, palette\u001B[38;5;241m=\u001B[39mPalette\u001B[38;5;241m.\u001B[39mWEB, colors\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m256\u001B[39m\n\u001B[0;32m    875\u001B[0m ):\n\u001B[0;32m    876\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    877\u001B[0m \u001B[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001B[39;00m\n\u001B[0;32m    878\u001B[0m \u001B[38;5;124;03m    method translates pixels through the palette.  If mode is\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    918\u001B[0m \u001B[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001B[39;00m\n\u001B[0;32m    919\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 921\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    923\u001B[0m     has_transparency \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtransparency\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    924\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m mode \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mP\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    925\u001B[0m         \u001B[38;5;66;03m# determine default mode\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\python_swe\\Lib\\site-packages\\PIL\\ImageFile.py:260\u001B[0m, in \u001B[0;36mImageFile.load\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    254\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\n\u001B[0;32m    255\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimage file is truncated \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    256\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(b)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m bytes not processed)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    257\u001B[0m         )\n\u001B[0;32m    259\u001B[0m b \u001B[38;5;241m=\u001B[39m b \u001B[38;5;241m+\u001B[39m s\n\u001B[1;32m--> 260\u001B[0m n, err_code \u001B[38;5;241m=\u001B[39m \u001B[43mdecoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    261\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    262\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Get CPU or GPU device for training\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device!\") \n",
    "\n",
    "# Create an instance of the model class and allocate it to the device\n",
    "model = Net(num_classes=len(classes)).to(device)\n",
    "\n",
    "# Use an \"Adam\"/\"SGD\" optimizer to adjust weights, \n",
    "# and weight decay for L2 regularization to avoid overfitting\n",
    "# (Ref: https://pytorch.org/docs/stable/optim.html#algorithms for details of supported algorithms)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Specify the loss criteria\n",
    "loss_criteria = nn.CrossEntropyLoss()\n",
    "\n",
    "# Track metrics in these arrays\n",
    "epoch_nums = []\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "# Training over 5 epochs (in a real scenario, you'd likely to use many more)\n",
    "epochs = 5\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train(model, device, train_loader, optimizer, epoch)\n",
    "    test_loss = test(model, device, test_loader)\n",
    "    epoch_nums.append(epoch)\n",
    "    training_loss.append(train_loss)\n",
    "    validation_loss.append(test_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-01T19:42:04.380430Z",
     "start_time": "2023-10-01T17:21:37.922708Z"
    }
   },
   "id": "86d0cabfa952ad2f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot to view the history loss\n",
    "plt.plot(epoch_nums, training_loss)\n",
    "plt.plot(epoch_nums, validation_loss)\n",
    "plt.title(\"Training V Validation Loss\")\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['training', 'validation'], loc='upper right')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-01T19:42:04.384431700Z"
    }
   },
   "id": "bcf852c577477d63"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate the model performance\n",
    "# Using Scikit-learn confusion matrix since Pytorch doesn't have an inbuilt method for that.\n",
    "from sklearn.metrics import (confusion_matrix, \n",
    "                             ConfusionMatrixDisplay) \n",
    "\n",
    "# Set the model to evaluate mode\n",
    "model.eval()\n",
    "\n",
    "# Get predictions for the test data and convert to numpy arrays for use with Scikit-learn\n",
    "print(\"Getting  predictions from test set...\")\n",
    "true_labels = []\n",
    "predictions = []\n",
    "\n",
    "\n",
    "for data, target in test_loader:\n",
    "    for label in target.cpu().data.numpy():\n",
    "        true_labels.append(label)\n",
    "    for prediction in model.cpu()(data).data.numpy().argmax(1):\n",
    "        predictions.append(prediction)\n",
    "                \n",
    "# Plot the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "# plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "ConfusionMatrixDisplay(cm).plot(cmap=plt.cm.Blues)\n",
    "# plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=45)\n",
    "plt.yticks(tick_marks, classes)\n",
    "plt.xlabel(\"Predicted Maize Disease Status\")\n",
    "plt.ylabel(\"Actual Maize Disease Status\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-01T19:42:04.385431600Z"
    }
   },
   "id": "7e89548fa7423e7d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "\n",
    "# Check if the models folder exists\n",
    "models_folder_path = Path(\"__file__\").cwd().parent / \"models\"\n",
    "if not models_folder_path.exists():\n",
    "    # If it doesn't exist, create it\n",
    "    models_folder_path.mkdir(parents=True)\n",
    "    print(f\"Folder '{models_folder_path}' created!\")\n",
    "else:\n",
    "    print(f\"Folder '{models_folder_path}' already exists!\")\n",
    "\n",
    "# Define the model file path\n",
    "model_file_path =  models_folder_path / \"maize_classifier.pt\"\n",
    "torch.save(model.state_dict(), \n",
    "          model_file_path)\n",
    "del model\n",
    "\n",
    "print(f\"Model saved as {model_file_path}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-01T19:42:04.393638900Z",
     "start_time": "2023-10-01T19:42:04.389635400Z"
    }
   },
   "id": "12ca94cbe93d1bf4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-01T19:42:04.392639500Z"
    }
   },
   "id": "ecd3e66e282b9e67"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
